{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "972abcb4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca5e1-717f-451f-b23f-ef411de8576f",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 7: Word embeddings and topic modeling \n",
    "**Due date: [Mar 24, 11:59 pm](https://github.com/UBC-CS/cpsc330-2024W2?tab=readme-ov-file#deliverable-due-dates-tentative).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5cb55-0576-4596-ba0e-88d0ad7c39f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b799e3-3d4e-4151-9123-9d3733a63ac3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da966a1-1d24-42a8-b959-e42202e6b824",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "**Please be aware that this homework assignment requires installation of several packages in your course environment. It's possible that you'll encounter installation challenges, which might be frustrating. However, remember that solving these issues is not wasting time but it is an essential skill for anyone aspiring to work in data science or machine learning.**\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W2/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb. If the pdf or html also fail to render on Gradescope, please create two files for your homework: hw6a.ipynb with Exercise 1 and hw6b.ipynb with Exercises 2 and 3 and submit these two files in your submission.  \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 18, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings work well on a variety of text classification tasks. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads the word vectors trained on Wikipedia using an algorithm called Glove. You'll need `gensim` package in your cpsc330 conda environment to run the code below. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in this pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1002-2451-4008-aa83-54618c757bb3",
   "metadata": {},
   "source": [
    "Now that we have GloVe Wiki vectors loaded in `glove_wiki_vectors`, let's explore the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Come up with a list of 4 words of your choice and find similar words to these words using `glove_wiki_vectors` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe4d0-4206-4747-9638-7782cca51d3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6205ebad-49a9-435a-bf84-ae57d29c2068",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\"pizza\", \"computer\", \"queen\", \"forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d59251b0-12c0-41cf-867b-3e5cb90f4d8d",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most similar words to 'pizza':\n",
      "  sandwich (similarity: 0.7488)\n",
      "  sandwiches (similarity: 0.7222)\n",
      "  bread (similarity: 0.7007)\n",
      "  pizzas (similarity: 0.7005)\n",
      "  burger (similarity: 0.6978)\n",
      "\n",
      "Most similar words to 'computer':\n",
      "  computers (similarity: 0.8752)\n",
      "  software (similarity: 0.8373)\n",
      "  technology (similarity: 0.7642)\n",
      "  pc (similarity: 0.7366)\n",
      "  hardware (similarity: 0.7290)\n",
      "\n",
      "Most similar words to 'queen':\n",
      "  princess (similarity: 0.7947)\n",
      "  king (similarity: 0.7508)\n",
      "  elizabeth (similarity: 0.7356)\n",
      "  royal (similarity: 0.7065)\n",
      "  lady (similarity: 0.7045)\n",
      "\n",
      "Most similar words to 'forest':\n",
      "  forests (similarity: 0.8435)\n",
      "  woodland (similarity: 0.7261)\n",
      "  rainforest (similarity: 0.6805)\n",
      "  wildlife (similarity: 0.6762)\n",
      "  habitat (similarity: 0.6734)\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(f\"\\nMost similar words to '{word}':\")\n",
    "    similar = glove_wiki_vectors.most_similar(word, topn=5)\n",
    "    for sim_word, score in similar:\n",
    "        print(f\"  {sim_word} (similarity: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24714da4-3ee5-424e-9a21-bd4b33de2e08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461c1d5-42ff-4267-bf04-e3642c5b40bb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of `glove_wiki_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b37d06f-81c7-4120-8dc7-2270105d5ecb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427682-97e3-4fa2-b2d6-84940fce9e27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7c8c6c4-7113-4cb2-98f8-d34ec1c632e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'coast' and 'shore': 0.7000\n",
      "Cosine similarity between 'clothes' and 'closet': 0.5463\n",
      "Cosine similarity between 'old' and 'new': 0.6432\n",
      "Cosine similarity between 'smart' and 'intelligent': 0.7553\n",
      "Cosine similarity between 'dog' and 'cat': 0.8798\n",
      "Cosine similarity between 'tree' and 'lawyer': 0.0767\n"
     ]
    }
   ],
   "source": [
    "for w1, w2 in word_pairs:\n",
    "    try:\n",
    "        similarity = glove_wiki_vectors.similarity(w1, w2)\n",
    "        print(f\"Cosine similarity between '{w1}' and '{w2}': {similarity:.4f}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"One or both the words ('{word1}', '{word2}') are not a part of the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a6420-ae43-4469-99e1-59a966238a43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9186e-ab44-43e5-8a96-7a1c4130b598",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Representation of all words in English\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. The `test_words` list below contains a few new words (called neologisms) and biomedical domain-specific abbreviations. Write code to check whether `glove_wiki_vectors` have representation for these words or not. \n",
    "> If a given word `word` is in the vocabulary, `word in glove_wiki_vectors` will return True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be6a6488-5139-43cb-a88c-b1d45df700cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    \"covididiot\",\n",
    "    \"fomo\",\n",
    "    \"frenemies\",\n",
    "    \"anthropause\",\n",
    "    \"photobomb\",\n",
    "    \"selfie\",\n",
    "    \"pxg\",  # Abbreviation for pseudoexfoliative glaucoma\n",
    "    \"pacg\",  # Abbreviation for primary angle closure glaucoma\n",
    "    \"cct\",  # Abbreviation for central corneal thickness\n",
    "    \"escc\",  # Abbreviation for esophageal squamous cell carcinoma\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfef3be-698f-4792-b7c3-46fac945e7f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44097c5d-97a0-450e-af88-73c9a4c90c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covididiot: Not in vocabulary\n",
      "fomo: Not in vocabulary\n",
      "frenemies: In vocabulary\n",
      "anthropause: Not in vocabulary\n",
      "photobomb: Not in vocabulary\n",
      "selfie: Not in vocabulary\n",
      "pxg: Not in vocabulary\n",
      "pacg: Not in vocabulary\n",
      "cct: In vocabulary\n",
      "escc: In vocabulary\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    in_vocab = word in glove_wiki_vectors\n",
    "    print(f\"{word}: {'In vocabulary' if in_vocab else 'Not in vocabulary'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2155dbe-979f-4e4d-bd4c-04d2a5f0be20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea8f10-8bfb-4698-b76c-60f5f8256d5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Stereotypes and biases in embeddings\n",
    "rubric={points}\n",
    "\n",
    "Word vectors contain lots of useful information. But they also contain stereotypes and biases of the texts they were trained on. In the lecture, we saw an example of gender bias in Google News word embeddings. Here we are using pre-trained embeddings trained on Wikipedia data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore whether there are any worrisome biases or stereotypes present in these embeddings by trying out at least 4 examples. You can use the following two methods or other methods of your choice to explore this. \n",
    "    - the `analogy` function below which gives word analogies (an example shown below)\n",
    "    - [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods (an example is shown below)\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a90c0cd-7cf2-4f82-a617-9a87c526281d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaeec14-64c1-4226-b60f-849489077ddd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examples of using analogy to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e914a9fa-e1e9-4182-a82d-41737020e44f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.773523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.718943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.682433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.675068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.672603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>0.664246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.652045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.639333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.638750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0        nurse  0.773523\n",
       "1    physician  0.718943\n",
       "2      doctors  0.682433\n",
       "3      patient  0.675068\n",
       "4      dentist  0.672603\n",
       "5     pregnant  0.664246\n",
       "6      medical  0.652045\n",
       "7      nursing  0.645348\n",
       "8       mother  0.639333\n",
       "9     hospital  0.638750"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fceb0128-3c9b-4674-ae40-da3d80f3f00c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14283238"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"aboriginal\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "774cb08e-95bc-458b-bc78-ee3fcf2b2f7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.351824"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bef6d-9a57-41a4-8a83-a65d7bc07783",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a227771e-f8b7-4bad-9881-eb5528f10e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias in beauty and gender:\n",
      "man : strong :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stronger</td>\n",
       "      <td>0.726966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weak</td>\n",
       "      <td>0.657428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robust</td>\n",
       "      <td>0.644944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strongest</td>\n",
       "      <td>0.634013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>despite</td>\n",
       "      <td>0.631509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>support</td>\n",
       "      <td>0.627726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>growing</td>\n",
       "      <td>0.625404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>concern</td>\n",
       "      <td>0.605287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>particularly</td>\n",
       "      <td>0.603681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reflected</td>\n",
       "      <td>0.600039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analogy word     Score\n",
       "0      stronger  0.726966\n",
       "1          weak  0.657428\n",
       "2        robust  0.644944\n",
       "3     strongest  0.634013\n",
       "4       despite  0.631509\n",
       "5       support  0.627726\n",
       "6       growing  0.625404\n",
       "7       concern  0.605287\n",
       "8  particularly  0.603681\n",
       "9     reflected  0.600039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Bias in beauty and gender:\")\n",
    "display(analogy(\"man\", \"strong\", \"woman\", model=glove_wiki_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6025b7c9-5ffd-4076-8d44-0440d01efe69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias in gender and family roles:\n",
      "man : boss :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bosses</td>\n",
       "      <td>0.649429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriend</td>\n",
       "      <td>0.639793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boyfriend</td>\n",
       "      <td>0.623138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>colleague</td>\n",
       "      <td>0.604826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lover</td>\n",
       "      <td>0.593473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.590123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ex</td>\n",
       "      <td>0.575455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>friend</td>\n",
       "      <td>0.565926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.555912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>housekeeper</td>\n",
       "      <td>0.527948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0       bosses  0.649429\n",
       "1   girlfriend  0.639793\n",
       "2    boyfriend  0.623138\n",
       "3    colleague  0.604826\n",
       "4        lover  0.593473\n",
       "5      husband  0.590123\n",
       "6           ex  0.575455\n",
       "7       friend  0.565926\n",
       "8         wife  0.555912\n",
       "9  housekeeper  0.527948"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Bias in gender and family roles:\")\n",
    "display(analogy(\"man\", \"boss\", \"woman\", model=glove_wiki_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "deaa74e9-858e-4b28-a7a8-6b6b522c1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias in religion and violence:\n",
      "christian : peace :: muslim : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>truce</td>\n",
       "      <td>0.679213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kashmir</td>\n",
       "      <td>0.660678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ceasefire</td>\n",
       "      <td>0.656898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conflict</td>\n",
       "      <td>0.656596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cease</td>\n",
       "      <td>0.645088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>palestinian</td>\n",
       "      <td>0.644061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sudan</td>\n",
       "      <td>0.633034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>darfur</td>\n",
       "      <td>0.628929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>palestinians</td>\n",
       "      <td>0.628015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sudanese</td>\n",
       "      <td>0.618921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analogy word     Score\n",
       "0         truce  0.679213\n",
       "1       kashmir  0.660678\n",
       "2     ceasefire  0.656898\n",
       "3      conflict  0.656596\n",
       "4         cease  0.645088\n",
       "5   palestinian  0.644061\n",
       "6         sudan  0.633034\n",
       "7        darfur  0.628929\n",
       "8  palestinians  0.628015\n",
       "9      sudanese  0.618921"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Bias in religion and violence:\")\n",
    "display(analogy(\"christian\", \"peace\", \"muslim\", model=glove_wiki_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95e306e7-26f3-4367-8935-0be49b86be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias in race and crime association:\n",
      "white : lawful :: black : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unlawful</td>\n",
       "      <td>0.703117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>legitimate</td>\n",
       "      <td>0.607818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unjust</td>\n",
       "      <td>0.545922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arbitrary</td>\n",
       "      <td>0.542710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unreasonable</td>\n",
       "      <td>0.538038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>capricious</td>\n",
       "      <td>0.521254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>legally</td>\n",
       "      <td>0.520878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>immoral</td>\n",
       "      <td>0.518508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>coercive</td>\n",
       "      <td>0.503806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cruel</td>\n",
       "      <td>0.502898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analogy word     Score\n",
       "0      unlawful  0.703117\n",
       "1    legitimate  0.607818\n",
       "2        unjust  0.545922\n",
       "3     arbitrary  0.542710\n",
       "4  unreasonable  0.538038\n",
       "5    capricious  0.521254\n",
       "6       legally  0.520878\n",
       "7       immoral  0.518508\n",
       "8      coercive  0.503806\n",
       "9         cruel  0.502898"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Bias in race and crime association:\")\n",
    "display(analogy(\"white\", \"lawful\", \"black\", model=glove_wiki_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474b6ac-25f3-45f1-97db-bfadf71d9f80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Discuss your observations from 1.4. Are there any worrisome biases in these embeddings trained on Wikipedia?   \n",
    "2. Give an example of how using embeddings with biases could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95ea8-80c0-4184-8b89-1fa1581404f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363f3ce",
   "metadata": {},
   "source": [
    "### **Key Takeaways on Bias in Word Embeddings**\n",
    "\n",
    "#### Observed Biases:\n",
    "1. **Gender Bias**  \n",
    "   Words like *\"woman\"* are frequently associated with caregiving roles such as *\"nurse\"* or *\"mother\"*, while *\"man\"* tends to be linked with traits like *\"strong\"* or professions like *\"doctor\"*, reflecting traditional gender stereotypes.\n",
    "\n",
    "2. **Cultural Bias**  \n",
    "   Female-associated terms, such as *\"queen\"*, are more often connected to words like *\"beautiful\"* or *\"glamorous\"*, emphasizing appearance over power or status, in contrast to male terms like *\"king\"*.\n",
    "\n",
    "3. **Racial Bias**  \n",
    "   Words like *\"white\"* show a stronger semantic association with terms like *\"success\"*, while words like *\"black\"* tend to have weaker or more negative associations, indicating implicit racial bias.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters:\n",
    "Pre-trained word embeddings reflect patterns in the data they are trained on, including societal stereotypes. These biases can carry over into machine learning models and affect their predictions and decisions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Potential Harms in Real-World Applications:\n",
    "- **Hiring Algorithms**: May reinforce existing gender or racial disparities in candidate evaluation.\n",
    "- **Healthcare Systems**: Risk of unequal treatment prioritization based on biased language data.\n",
    "- **Content Moderation**: Biased embeddings may fail to accurately detect or classify offensive content across different groups.\n",
    "- **Generative AI Models**: May produce outputs that unintentionally reinforce stereotypes and biased narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb13f7-e08c-4f8d-86c9-b1602cc8a5b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Topic modeling \n",
    "\n",
    "The goal of topic modeling is discovering high-level themes in a large collection of texts. \n",
    "\n",
    "In this homework, you will explore topics in [the 20 newsgroups text dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) using `scikit-learn`'s `LatentDirichletAllocation` (LDA) model. \n",
    "\n",
    "Usually, topic modeling is used for discovering abstract \"topics\" that occur in a collection of documents when you do not know the actual topics present in the documents. But 20 newsgroups text dataset is labeled with categories (e.g., sports, hardware, religion), and you will be able to cross-check the topics discovered by your model with these available topics. \n",
    "\n",
    "The starter code below loads the train and test portion of the data and convert the train portion into a pandas DataFrame. For speed, we will only consider documents with the following 8 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "327ca91e-cf57-4481-b4cc-46c29a9849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9afa3b30-7f32-48ec-8291-69c964a38c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name  \n",
       "0        talk.politics.guns  \n",
       "1             comp.graphics  \n",
       "2             comp.graphics  \n",
       "3            comp.windows.x  \n",
       "4     talk.politics.mideast  \n",
       "...                     ...  \n",
       "4558          comp.graphics  \n",
       "4559         comp.windows.x  \n",
       "4560     talk.politics.guns  \n",
       "4561         comp.windows.x  \n",
       "4562  talk.politics.mideast  \n",
       "\n",
       "[4563 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"rec.sport.hockey\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.windows.x\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.guns\",\n",
    "]  # We'll only consider these categories out of 20 categories for speed.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=cats\n",
    ")\n",
    "X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target\n",
    "df = pd.DataFrame(X_news_train, columns=[\"text\"])\n",
    "df[\"target\"] = y_news_train\n",
    "df[\"target_name\"] = [\n",
    "    newsgroups_train.target_names[target] for target in newsgroups_train.target\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8095c853-d4d5-49a3-bda0-129ffd2f690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cdcfc-7545-4a40-a9b3-34b58bb38365",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21b066-8de6-42a7-8e4e-097fd8727367",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Preprocessing using [spaCy](https://spacy.io/)\n",
    "rubric={points}\n",
    "\n",
    "Preprocessing is a crucial step before carrying out topic modeling and it markedly affects topic modeling results. In this exercise, you'll prepare the data using [spaCy](https://spacy.io/) for topic modeling. \n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "- Write code using [spaCy](https://spacy.io/) to preprocess the `text` column in the given dataframe `df` and save the processed text in a new column called `text_pp` within the same dataframe.\n",
    "\n",
    "If you do not have spaCy in your course environment, you'll have to [install it](https://spacy.io/usage) and download the pretrained model en_core_web_md. \n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "\n",
    "Note that there is no such thing as \"perfect\" preprocessing. You'll have to make your own judgments and decisions on which tokens are likely to be more informative for the given task. Some common text preprocessing steps for topic modeling include: \n",
    "- getting rid of slashes, new-line characters, or any other non-informative characters\n",
    "- sentence segmentation and tokenization      \n",
    "- replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "- getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "- excluding stopwords and punctuation \n",
    "- lemmatization\n",
    "\n",
    "\n",
    "> Check out [these available attributes](https://spacy.io/api/token#attributes) for `token` in spaCy which might help you with preprocessing. \n",
    "\n",
    "> You can also get rid of words with specific POS tags. [Here](https://universaldependencies.org/u/pos/) is the list of part-of-speech tags used in spaCy. \n",
    "\n",
    "> You may have to use regex to clean text before passing it to spaCy. Also, you might have to go back and forth between preprocessing in this exercise and and topic modeling in Exercise 2 before finalizing preprocessing steps. \n",
    "\n",
    "> Note that preprocessing the corpus might take some time. So here are a couple of suggestions: 1) During the debugging phase, work on a smaller subset of the data. 2) Once you finalize the preprocessing part, you might want to save the preprocessed data in a CSV and work with this CSV so that you don't run the preprocessing part every time you run the notebook. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6816e7a3-d6d4-4bef-81f4-9b4fdf638175",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dbe6-2353-4c5a-80bf-2b884a170a29",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8be7c802-d10b-4929-9664-34274299e884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_texts_with_pipe(texts):\n",
    "    processed_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=50):  # Process in batches\n",
    "        tokens = [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.like_url\n",
    "            and not token.like_email\n",
    "            and not token.like_num\n",
    "            and token.is_alpha\n",
    "        ]\n",
    "        processed_texts.append(\" \".join(tokens))\n",
    "    return processed_texts\n",
    "\n",
    "def clean_raw_text(text):\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"EMAIL\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"URL\", text)\n",
    "    text = re.sub(r\"\\d+\", \"NUM\", text)\n",
    "    text = re.sub(r\"[\\r\\n\\t\\/\\\\]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df[\"text_cleaned\"] = df[\"text\"].apply(clean_raw_text)\n",
    "\n",
    "df[\"text_pp\"] = preprocess_texts_with_pipe(df[\"text_cleaned\"])\n",
    "\n",
    "df.to_csv(\"preprocessed_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c2b633b-6601-4b83-8668-800a99d3ba20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>Actuallay I don't, but on the other hand I do...</td>\n",
       "      <td>actuallay hand support idea have newsgroup asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>The following problem is really bugging me, an...</td>\n",
       "      <td>follow problem bug appreciate help create wind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>This is the latest from UPI        Foreign...</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi,\\n   I'd like to subscribe to Leadership Ma...</td>\n",
       "      <td>5</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>Hi,    I'd like to subscribe to Leadership Mag...</td>\n",
       "      <td>hi like subscribe leadership magazine wonder d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "2  \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3  The following problem is really bugging me,\\na...       2   \n",
       "4  \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "5  Hi,\\n   I'd like to subscribe to Leadership Ma...       5   \n",
       "\n",
       "              target_name                                       text_cleaned  \\\n",
       "2           comp.graphics   Actuallay I don't, but on the other hand I do...   \n",
       "3          comp.windows.x  The following problem is really bugging me, an...   \n",
       "4   talk.politics.mideast      This is the latest from UPI        Foreign...   \n",
       "5  soc.religion.christian  Hi,    I'd like to subscribe to Leadership Mag...   \n",
       "\n",
       "                                             text_pp  \n",
       "2  actuallay hand support idea have newsgroup asp...  \n",
       "3  follow problem bug appreciate help create wind...  \n",
       "4  late upi foreign ministry spokesman ferhat ata...  \n",
       "5  hi like subscribe leadership magazine wonder d...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbad26-84c1-41ed-8201-0843924b3166",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f9bd5-31ef-43dd-9ec7-e0e1cd9a9714",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Justification\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Outline the preprocessing steps you carried out in the previous exercise (bullet point format is fine), providing a brief justification when necessary. \n",
    "\n",
    "> You might want to wait to answer this question till you are done with Exercise 2 and you have finalized the preprocessing steps in 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8df885-0fb7-4ec7-bef6-bb0c3cd82e9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ad85e",
   "metadata": {},
   "source": [
    "### **Preprocessing Steps**\n",
    "\n",
    "- **Initial Text Cleaning (Regex)**:  \n",
    "  Replaced noisy patterns such as email addresses, URLs, numeric values, and escape characters with generic placeholders.  \n",
    "  *Prevents rare, non-informative tokens from distorting topic distributions.*\n",
    "\n",
    "- **Tokenization**:  \n",
    "  Used `spaCy`'s `nlp.pipe()` to efficiently split the text into individual tokens.  \n",
    "  *Ensures consistent and efficient document parsing.*\n",
    "\n",
    "- **Lemmatization**:  \n",
    "  Converted each token to its lemma (base form) and lowercased it.  \n",
    "  *Groups word variants (e.g., â€œrunningâ€ and â€œrunâ€) to improve topic clustering.*\n",
    "\n",
    "- **Stopword Removal**:  \n",
    "  Removed common English stopwords (e.g., \"and\", \"is\", \"but\").  \n",
    "  *Eliminates high-frequency but low-value words.*\n",
    "\n",
    "- **Punctuation & Special Token Filtering**:  \n",
    "  Removed punctuation, numeric tokens, and other non-alphabetic strings.  \n",
    "  *Reduces clutter and keeps the token set semantically meaningful.*\n",
    "\n",
    "- **Batch Processing with `nlp.pipe()`**:  \n",
    "  Used spaCyâ€™s optimized pipeline to process texts in batches.  \n",
    "  *Improves runtime performance on large corpora.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22f0e-2526-4f83-94ee-a625c7a5ed04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080f91c-6801-4b89-9fbc-1b574b09915c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Build a topic model using sklearn's LatentDirichletAllocation\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build LDA models on the preprocessed data using using [sklearn's `LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and random state 42. Experiment with a few values for the number of topics (`n_components`). Pick a reasonable number for the number of topics and briefly justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e472711-3c92-4b82-a31d-5d4bf55dbf2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b624ef",
   "metadata": {},
   "source": [
    "### **Justification for Number of Topics**\n",
    "\n",
    "- **Exploration**:  \n",
    "  We trained multiple LDA models with different topic counts (`n_components = 5, 8, 10, 15`) using `random_state=42`.\n",
    "\n",
    "- **Clarity & Coherence**:  \n",
    "  - Models with **fewer topics** (e.g., 5) grouped unrelated themes together, resulting in overly broad topics.  \n",
    "  - Models with **too many topics** (e.g., 10 or 15) introduced overlap and fragmentation, making interpretation more difficult.\n",
    "\n",
    "- **Final Choice â€“ 8 Topics**:  \n",
    "  - **8 topics** aligned well with the known number of categories in our dataset (e.g., religion, sports, politics, tech).  \n",
    "  - The topics generated were coherent, distinguishable, and easy to label based on top keywords.  \n",
    "  - This number balanced interpretability with sufficient thematic granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88908be8-85df-40bf-9d6d-0b48948a7137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bed16e6f-2706-463e-80cc-c64d2b75a296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topic 1': ['team',\n",
       "  'game',\n",
       "  'year',\n",
       "  'good',\n",
       "  'run',\n",
       "  'player',\n",
       "  'win',\n",
       "  'like',\n",
       "  'play',\n",
       "  'think'],\n",
       " 'Topic 2': ['num',\n",
       "  'period',\n",
       "  'email',\n",
       "  'la',\n",
       "  'play',\n",
       "  'team',\n",
       "  'pts',\n",
       "  'new',\n",
       "  'pp',\n",
       "  'power'],\n",
       " 'Topic 3': ['num',\n",
       "  'people',\n",
       "  'armenian',\n",
       "  'turkish',\n",
       "  'jews',\n",
       "  'armenians',\n",
       "  'claim',\n",
       "  'turkey',\n",
       "  'human',\n",
       "  'state'],\n",
       " 'Topic 4': ['say',\n",
       "  'num',\n",
       "  'know',\n",
       "  'people',\n",
       "  'come',\n",
       "  'tell',\n",
       "  'think',\n",
       "  'like',\n",
       "  'time',\n",
       "  'want'],\n",
       " 'Topic 5': ['num',\n",
       "  'email',\n",
       "  'file',\n",
       "  'program',\n",
       "  'use',\n",
       "  'image',\n",
       "  'window',\n",
       "  'available',\n",
       "  'include',\n",
       "  'server'],\n",
       " 'Topic 6': ['god',\n",
       "  'jesus',\n",
       "  'believe',\n",
       "  'people',\n",
       "  'think',\n",
       "  'know',\n",
       "  'say',\n",
       "  'num',\n",
       "  'thing',\n",
       "  'bible'],\n",
       " 'Topic 7': ['game',\n",
       "  'year',\n",
       "  'think',\n",
       "  'good',\n",
       "  'play',\n",
       "  'team',\n",
       "  'time',\n",
       "  'season',\n",
       "  'player',\n",
       "  'like'],\n",
       " 'Topic 8': ['gun',\n",
       "  'israel',\n",
       "  'people',\n",
       "  'right',\n",
       "  'state',\n",
       "  'law',\n",
       "  'weapon',\n",
       "  'num',\n",
       "  'think',\n",
       "  'firearm']}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df[\"text_pp\"])\n",
    "\n",
    "topic_numbers = [5, 8, 10, 15]\n",
    "lda_models = {}\n",
    "\n",
    "for n in topic_numbers:\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=n,\n",
    "        random_state=42,\n",
    "        learning_method=\"batch\",\n",
    "        max_iter=10\n",
    "    )\n",
    "    lda.fit(dtm)\n",
    "    lda_models[n] = lda\n",
    "\n",
    "selected_n_topics = 8\n",
    "lda_model = lda_models[selected_n_topics]\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
    "        topics[f\"Topic {topic_idx + 1}\"] = top_words\n",
    "    return topics\n",
    "\n",
    "num_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "topics = display_topics(lda_model, feature_names, num_top_words)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1afbdb-b419-4d17-a7fa-0dfd9e618f15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ff43-188c-4d9a-8404-691cd563dd9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 Exploring word topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For the number of topics you picked in the previous exercise, show top 10 words for each of your topics and suggest labels for each of the topics (similar to how we came up with labels \"health and nutrition\", \"fashion\", and \"machine learning\" in the toy example we saw in class). \n",
    "\n",
    "> If your topics do not make much sense, you might have to go back to preprocessing in Exercise 2.1, improve it, and train your LDA model again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe9de-c9d9-41a1-bf5a-10220ce6fa38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "967136f3-5746-4568-8d2d-d48c4d3be36c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame.from_dict(\n",
    "    topics,\n",
    "    orient='index',\n",
    "    columns=[f\"Word {i+1}\" for i in range(num_top_words)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18caef39-4161-438b-90f2-99337f39dcd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Word 1  Word 2    Word 3   Word 4 Word 5     Word 6  Word 7  \\\n",
      "Topic 1   team    game      year     good    run     player     win   \n",
      "Topic 2    num  period     email       la   play       team     pts   \n",
      "Topic 3    num  people  armenian  turkish   jews  armenians   claim   \n",
      "Topic 4    say     num      know   people   come       tell   think   \n",
      "Topic 5    num   email      file  program    use      image  window   \n",
      "Topic 6    god   jesus   believe   people  think       know     say   \n",
      "Topic 7   game    year     think     good   play       team    time   \n",
      "Topic 8    gun  israel    people    right  state        law  weapon   \n",
      "\n",
      "            Word 8   Word 9  Word 10  \n",
      "Topic 1       like     play    think  \n",
      "Topic 2        new       pp    power  \n",
      "Topic 3     turkey    human    state  \n",
      "Topic 4       like     time     want  \n",
      "Topic 5  available  include   server  \n",
      "Topic 6        num    thing    bible  \n",
      "Topic 7     season   player     like  \n",
      "Topic 8        num    think  firearm  \n"
     ]
    }
   ],
   "source": [
    "print(topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a5220-00ea-4f83-9bb4-d7c2a7090843",
   "metadata": {},
   "source": [
    "| **Topic #** | **Top Words** | **Suggested Label** |\n",
    "|------------|----------------|---------------------|\n",
    "| **Topic 1** | team, game, year, good, run, player, win, like, play, think | Sports (General) |\n",
    "| **Topic 2** | num, period, email, la, play, team, pts, new, pp, power | Emails / Sports Statistics |\n",
    "| **Topic 3** | num, people, armenian, turkish, jews, armenians, claim, turkey, human, state | Ethnic and Geopolitical Conflict |\n",
    "| **Topic 4** | say, num, people, come, tell, think, time, like, want | General Discussion / Opinion |\n",
    "| **Topic 5** | num, email, file, program, use, image, window, available, include, server | Technology / File Systems |\n",
    "| **Topic 6** | god, jesus, believe, people, think, know, say, num, thing, bible | Religion and Belief |\n",
    "| **Topic 7** | game, year, think, good, play, team, time, season, player, like | Sports |\n",
    "| **Topic 8** | gun, israel, people, right, state, law, weapon, num, think, firearm | Guns, Politics, and Law |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cbbb9-1057-43ef-ac63-842e3e4cb43c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53117b-897c-4eba-b1bd-dfedcb3d35ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Exploring document topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show the document topic assignment of the first five documents from `df`. \n",
    "2. Comment on the document topic assignment of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12934a67-c48d-4c42-a856-248615c6202b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f09783a-5644-46ac-bb2f-d60947a13d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  You know, I was reading 18 U.S.C. 922 and some...   \n",
      "1  \\n\\n\\nIt's not a bad question: I don't have an...   \n",
      "2  \\nActuallay I don't, but on the other hand I d...   \n",
      "3  The following problem is really bugging me,\\na...   \n",
      "4  \\n\\n  This is the latest from UPI \\n\\n     For...   \n",
      "\n",
      "                                             text_pp  Dominant_Topic  \\\n",
      "0  know read num num sence wonder help num num pr...               8   \n",
      "1  bad question ref list algorithm think bit hard...               5   \n",
      "2  actuallay hand support idea have newsgroup asp...               6   \n",
      "3  follow problem bug appreciate help create wind...               5   \n",
      "4  late upi foreign ministry spokesman ferhat ata...               3   \n",
      "\n",
      "   Topic_Probability  \n",
      "0           0.889240  \n",
      "1           0.988151  \n",
      "2           0.576054  \n",
      "3           0.831263  \n",
      "4           0.730968  \n"
     ]
    }
   ],
   "source": [
    "doc_topic_matrix = lda_model.transform(dtm)\n",
    "\n",
    "df[\"Dominant_Topic\"] = np.argmax(doc_topic_matrix, axis=1) + 1  # +1 to match \"Topic 1\" style indexing\n",
    "df[\"Topic_Probability\"] = np.max(doc_topic_matrix, axis=1)\n",
    "\n",
    "first_five_docs = df[[\"text\", \"text_pp\", \"Dominant_Topic\", \"Topic_Probability\"]].head()\n",
    "print(first_five_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a654f5-7a0c-4e5b-96db-f1295946a3ad",
   "metadata": {},
   "source": [
    "### **Document Topic Interpretation**\n",
    "\n",
    "#### 1. **Dominant Topic Interpretation:**\n",
    "\n",
    "- **Document 0**: Assigned to **Topic 8** with **high confidence (0.89)**. The text discusses U.S. law (e.g., 18 U.S.C. 922), which aligns well with Topic 8â€™s focus on **guns, law, and political rights**.\n",
    "  \n",
    "- **Document 1**: Assigned to **Topic 5** with **very high confidence (0.99)**. The keywords *algorithm*, *list*, and *think* suggest a strong alignment with **computing or programming-related content**.\n",
    "\n",
    "- **Document 2**: Assigned to **Topic 6** with **moderate confidence (0.58)**. Although not highly confident, the textâ€™s structure and terms like *support*, *idea*, and *newsgroup* could relate to **online forums or belief systems**, which fits with Topic 6â€™s broader theme around **opinions or social discourse**.\n",
    "\n",
    "- **Document 3**: Assigned to **Topic 5** with **high confidence (0.83)**. It discusses a technical bug and solutions, which matches well with **software, user interface issues, or system troubleshooting**.\n",
    "\n",
    "- **Document 4**: Assigned to **Topic 3** with **good confidence (0.73)**. The document refers to foreign affairs and UPI news, likely matching Topic 3â€™s focus on **geopolitical or international conflict themes**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Strengths:**\n",
    "\n",
    "- The model identifies **computing and law-related documents** with high clarity and confidence (e.g., Documents 0, 1, and 3).\n",
    "- **Topic 3** successfully captures political/geopolitical content in Document 4, showing that domain-specific language is being clustered effectively.\n",
    "- Confidence scores above **0.80** in 3 out of 5 cases suggest strong topic alignment.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Weaknesses:**\n",
    "\n",
    "- **Document 2** has a **lower confidence score (~0.58)**, indicating weaker topic clarity â€” possibly due to **generic or conversational phrasing**.\n",
    "- Some overlap between topics involving **technology, support, or community-related vocabulary** may reduce distinction (e.g., Document 2 vs. Document 3).\n",
    "- Topics involving **broad social themes** may be harder to isolate without stricter preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Recommendations:**\n",
    "\n",
    "- Apply additional text cleaning (e.g., removing repeated â€œnumâ€-like placeholders) to improve clarity in documents like Document 2.\n",
    "- Explore **part-of-speech (POS) filtering** to retain more informative tokens (e.g., nouns and verbs only).\n",
    "- Consider **splitting broad topics** or increasing the number of topics slightly if you notice consistent overlap between categories like community support vs. technical bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e21cac-5f1a-4480-b683-47c3b41a1199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42685c5-01cb-4495-aae6-f803d6f75172",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Short answer questions \n",
    "<hr>\n",
    "\n",
    "rubric={points}\n",
    "\n",
    "1. Briefly explain how content-based filtering works in the context of recommender systems. \n",
    "2. Discuss at least two negative consequences of recommender systems.\n",
    "3. What is transfer learning in natural language processing? Briefly explain.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01807641-531e-454a-9374-fc8ad04bc30b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864fe617",
   "metadata": {},
   "source": [
    "#### **1. How Content-Based Filtering Works:**\n",
    "\n",
    "Content-based filtering recommends items to a user by analyzing the attributes of items they have previously interacted with or liked. It builds a **user profile** based on features of those items (e.g., genre, keywords, tags), then compares that profile to other items in the system and suggests ones with similar content.\n",
    "\n",
    "For example, if a user watches several sci-fi movies, the system recommends other sci-fi movies based on shared features like â€œspace,â€ â€œfuturistic,â€ or â€œAI.â€\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Negative Consequences of Recommender Systems:**\n",
    "\n",
    "- **Filter Bubbles & Echo Chambers**:  \n",
    "  Recommender systems often reinforce a userâ€™s existing preferences, exposing them to only similar viewpoints or content. This can lead to **narrowed perspectives** and lack of diversity in information.\n",
    "\n",
    "- **Bias & Inequality**:  \n",
    "  Systems may unintentionally favor popular or well-represented items, making it harder for **niche or minority content** to get visibility. This can amplify societal biases and reduce fairness in recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. What is Transfer Learning in NLP?**\n",
    "\n",
    "Transfer learning in NLP involves using a **pre-trained language model** (e.g., BERT, GPT) that has already learned general language patterns from a large corpus, and then **fine-tuning** it on a specific task like sentiment analysis or question answering. This approach leverages prior knowledge to achieve better performance with less task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929238-7f2c-421e-bb9a-32f0debf0636",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d6d11-0ef5-4315-9854-f64a427e62af",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491f1d3-dd84-4ed2-b2b3-9148ac8df7a5",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
